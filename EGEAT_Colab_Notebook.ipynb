{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGEAT: Ensemble Gradient-based Ensemble Adversarial Training\n",
    "\n",
    "This notebook runs the complete EGEAT experiment pipeline including:\n",
    "- Ensemble model training\n",
    "- Adversarial training with geometric regularization\n",
    "- Comprehensive evaluation metrics\n",
    "- Visualization of all diagrams and results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision numpy pandas matplotlib seaborn tqdm scikit-learn imageio -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set up paths for Colab\n",
    "WORK_DIR = '/content/EGEAT'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('src/attacks', exist_ok=True)\n",
    "os.makedirs('src/evaluation', exist_ok=True)\n",
    "os.makedirs('src/models', exist_ok=True)\n",
    "os.makedirs('src/training', exist_ok=True)\n",
    "os.makedirs('src/utils', exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Source Code Setup\n",
    "\n",
    "The following cells contain all the source code from the project organized by module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Configuration Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/config.py\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Optional\n",
    "import argparse\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 2e-4\n",
    "    epochs: int = 5\n",
    "    lambda_geom: float = 0.1\n",
    "    lambda_soup: float = 0.05\n",
    "    epsilon: float = 8/255\n",
    "    use_mixed_precision: bool = False\n",
    "    num_workers: int = 2\n",
    "    pin_memory: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_type: str = 'SimpleCNN'\n",
    "    input_channels: int = 3\n",
    "    num_classes: int = 10\n",
    "    ensemble_size: int = 5\n",
    "\n",
    "@dataclass\n",
    "class AttackConfig:\n",
    "    epsilon: float = 8/255\n",
    "    alpha: float = 2/255\n",
    "    pgd_iters: int = 10\n",
    "    cw_iters: int = 50\n",
    "    cw_c: float = 1e-2\n",
    "    cw_lr: float = 0.01\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    max_batches: int = 10\n",
    "    n_bins_ece: int = 15\n",
    "    loss_landscape_grid_n: int = 21\n",
    "    loss_landscape_radius: float = 1.0\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = 'cifar10'\n",
    "    val_split: float = 0.1\n",
    "    augment: bool = True\n",
    "    data_path: str = './data'\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    experiment_name: str = 'egeat_experiment'\n",
    "    seed: int = 42\n",
    "    device: Optional[str] = None\n",
    "    save_dir: str = 'results'\n",
    "    resume: bool = False\n",
    "    checkpoint_dir: Optional[str] = None\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    attack: AttackConfig = field(default_factory=AttackConfig)\n",
    "    evaluation: EvaluationConfig = field(default_factory=EvaluationConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        training = TrainingConfig(**d.pop('training', {}))\n",
    "        model = ModelConfig(**d.pop('model', {}))\n",
    "        attack = AttackConfig(**d.pop('attack', {}))\n",
    "        evaluation = EvaluationConfig(**d.pop('evaluation', {}))\n",
    "        data = DataConfig(**d.pop('data', {}))\n",
    "        return cls(training=training, model=model, attack=attack, evaluation=evaluation, data=data, **d)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        with open(path, 'r') as f:\n",
    "            d = json.load(f)\n",
    "        return cls.from_dict(d)\n",
    "\n",
    "def get_device(device_str=None, verbose=False):\n",
    "    if device_str is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device = torch.device(device_str)\n",
    "    if verbose:\n",
    "        print(f\"Using device: {device}\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/models/cnn.py\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.fc1 = nn.Linear(128*4*4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DCGAN_CNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128*8*8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils/data_loader.py\n",
    "def seed_everything(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_mnist_loaders(batch_size=128, val_split=0.1, num_workers=2, pin_memory=True, seed=123):\n",
    "    seed_everything(seed)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    dataset = datasets.MNIST(root='./data/MNIST', train=True, download=True, transform=transform)\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
    "    test_dataset = datasets.MNIST(root='./data/MNIST', train=False, download=True, transform=transform)\n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n",
    "        DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory),\n",
    "        DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory),\n",
    "    )\n",
    "\n",
    "def get_cifar10_loaders(batch_size=128, val_split=0.1, num_workers=2, pin_memory=True, augment=True, seed=123):\n",
    "    seed_everything(seed)\n",
    "    if augment:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
    "    test_dataset = datasets.CIFAR10(root='./data/CIFAR10', train=False, download=True, transform=transform_test)\n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n",
    "        DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory),\n",
    "        DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training Module (EGEAT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/training/train_egeat.py\n",
    "def exact_delta_star(grad, eps, norm='inf'):\n",
    "    if norm == 'inf':\n",
    "        return eps * grad.sign()\n",
    "    g_flat = grad.view(grad.size(0), -1)\n",
    "    if norm == 2:\n",
    "        g_norm = g_flat.norm(p=2, dim=1).view(-1, 1, 1, 1) + 1e-12\n",
    "        return eps * grad / g_norm\n",
    "    if norm == 1:\n",
    "        g_norm = g_flat.abs().max(dim=1)[0].view(-1, 1, 1, 1) + 1e-12\n",
    "        return eps * grad / g_norm\n",
    "    raise ValueError(\"Unsupported norm type.\")\n",
    "\n",
    "def gradient_subspace_penalty(model, x, y, ensemble_models):\n",
    "    if not ensemble_models or len(ensemble_models) == 0:\n",
    "        return 0.0\n",
    "    grads = []\n",
    "    x_req = x.clone().detach().requires_grad_(True)\n",
    "    out = model(x_req)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    g = torch.autograd.grad(loss, x_req)[0].view(x.size(0), -1)\n",
    "    grads.append(g)\n",
    "    for m in ensemble_models:\n",
    "        m.eval()\n",
    "        x_snap = x.clone().detach().requires_grad_(True)\n",
    "        out_snap = m(x_snap)\n",
    "        loss_snap = F.cross_entropy(out_snap, y)\n",
    "        g_snap = torch.autograd.grad(loss_snap, x_snap)[0].view(x.size(0), -1)\n",
    "        grads.append(g_snap)\n",
    "    L = len(grads)\n",
    "    penalty = 0.0\n",
    "    for i in range(L):\n",
    "        for j in range(i + 1, L):\n",
    "            Gi = grads[i]\n",
    "            Gj = grads[j]\n",
    "            num = (Gi * Gj).sum(dim=1).mean()\n",
    "            den = (Gi.norm(p=2, dim=1) * Gj.norm(p=2, dim=1)).mean() + 1e-12\n",
    "            penalty += num / den\n",
    "    return penalty / (L * (L - 1) / 2)\n",
    "\n",
    "def compute_theta_soup(ensemble_snapshots):\n",
    "    if ensemble_snapshots is None or len(ensemble_snapshots) == 0:\n",
    "        return None\n",
    "    soup = deepcopy(ensemble_snapshots[0])\n",
    "    for k in soup.keys():\n",
    "        for s in ensemble_snapshots[1:]:\n",
    "            soup[k] += s[k]\n",
    "        soup[k] /= len(ensemble_snapshots)\n",
    "    return soup\n",
    "\n",
    "def train_egeat_epoch(model, dataloader, optimizer, device='cuda', lambda_geom=0.1, lambda_soup=0.05, epsilon=8/255, p_norm='inf', ensemble_snapshots=None, use_mixed_precision=False):\n",
    "    model.train()\n",
    "    theta_soup = compute_theta_soup(ensemble_snapshots)\n",
    "    total_loss, total_adv, total_geom, total_soup = 0.0, 0.0, 0.0, 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler() if use_mixed_precision and device.type == 'cuda' else None\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x.requires_grad_(True)\n",
    "        \n",
    "        if use_mixed_precision and scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(x)\n",
    "                loss_clean = F.cross_entropy(logits, y)\n",
    "                grad_x = torch.autograd.grad(loss_clean, x, create_graph=False)[0]\n",
    "                delta_star = exact_delta_star(grad_x, epsilon, norm=p_norm)\n",
    "                x_adv = torch.clamp(x + delta_star, 0.0, 1.0)\n",
    "                logits_adv = model(x_adv)\n",
    "                L_adv = F.cross_entropy(logits_adv, y)\n",
    "                L_geom = gradient_subspace_penalty(model, x.detach(), y, ensemble_snapshots)\n",
    "                L_soup = 0.0\n",
    "                if theta_soup is not None:\n",
    "                    for p, key in zip(model.parameters(), theta_soup.keys()):\n",
    "                        p_s = theta_soup[key].to(device)\n",
    "                        L_soup += ((p - p_s) ** 2).sum()\n",
    "                L_total = L_adv + lambda_geom * L_geom + lambda_soup * L_soup\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(L_total).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(x)\n",
    "            loss_clean = F.cross_entropy(logits, y)\n",
    "            grad_x = torch.autograd.grad(loss_clean, x, create_graph=False)[0]\n",
    "            delta_star = exact_delta_star(grad_x, epsilon, norm=p_norm)\n",
    "            x_adv = torch.clamp(x + delta_star, 0.0, 1.0)\n",
    "            logits_adv = model(x_adv)\n",
    "            L_adv = F.cross_entropy(logits_adv, y)\n",
    "            L_geom = gradient_subspace_penalty(model, x.detach(), y, ensemble_snapshots)\n",
    "            L_soup = 0.0\n",
    "            if theta_soup is not None:\n",
    "                for p, key in zip(model.parameters(), theta_soup.keys()):\n",
    "                    p_s = theta_soup[key].to(device)\n",
    "                    L_soup += ((p - p_s) ** 2).sum()\n",
    "            L_total = L_adv + lambda_geom * L_geom + lambda_soup * L_soup\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            L_total.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += L_total.item()\n",
    "        total_adv += L_adv.item()\n",
    "        total_geom += L_geom if isinstance(L_geom, float) else L_geom.item()\n",
    "        total_soup += L_soup if isinstance(L_soup, float) else L_soup.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / n_batches,\n",
    "        'adv_loss': total_adv / n_batches,\n",
    "        'geom_loss': total_geom / n_batches,\n",
    "        'soup_loss': total_soup / n_batches\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Attack Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/attacks/pgd.py\n",
    "def pgd_attack(\n",
    "    model, x, y, epsilon=8/255, alpha=2/255, iters=10, device=None, norm='inf', random_start=True, targeted=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples using PGD (Projected Gradient Descent).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    model.eval()\n",
    "    x = x.clone().detach().to(device)\n",
    "    y = y.to(device)\n",
    "    x_orig = x.clone()\n",
    "    \n",
    "    # Initialize adversarial example\n",
    "    if random_start:\n",
    "        if norm == 'inf':\n",
    "            x_adv = x + torch.empty_like(x).uniform_(-epsilon, epsilon)\n",
    "        else:  # L2 norm\n",
    "            delta = torch.randn_like(x)\n",
    "            delta_norm = delta.view(delta.size(0), -1).norm(p=2, dim=1, keepdim=True)\n",
    "            delta = delta / (delta_norm.view(-1, 1, 1, 1) + 1e-10) * epsilon\n",
    "            x_adv = x + delta\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    else:\n",
    "        x_adv = x.clone()\n",
    "    \n",
    "    # PGD iterations\n",
    "    for i in range(iters):\n",
    "        x_adv.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(x_adv)\n",
    "        loss = nn.CrossEntropyLoss()(output, y)\n",
    "        \n",
    "        if targeted:\n",
    "            loss = -loss  # Minimize loss for targeted attack\n",
    "        \n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad.detach()\n",
    "        \n",
    "        # Update adversarial example\n",
    "        if norm == 'inf':\n",
    "            # L_inf: sign of gradient\n",
    "            x_adv = x_adv + alpha * grad.sign()\n",
    "            # Project back to epsilon-ball\n",
    "            x_adv = torch.min(torch.max(x_adv, x_orig - epsilon), x_orig + epsilon)\n",
    "        else:  # L2 norm\n",
    "            # L2: normalized gradient\n",
    "            grad_norm = grad.view(grad.size(0), -1).norm(p=2, dim=1, keepdim=True)\n",
    "            grad_normalized = grad / (grad_norm.view(-1, 1, 1, 1) + 1e-10)\n",
    "            x_adv = x_adv + alpha * grad_normalized\n",
    "            # Project back to epsilon-ball\n",
    "            delta = x_adv - x_orig\n",
    "            delta_norm = delta.view(delta.size(0), -1).norm(p=2, dim=1, keepdim=True)\n",
    "            delta_normalized = delta / (delta_norm.view(-1, 1, 1, 1) + 1e-10)\n",
    "            x_adv = x_orig + delta_normalized * torch.minimum(\n",
    "                delta_norm.view(-1, 1, 1, 1) / epsilon,\n",
    "                torch.ones_like(delta_norm.view(-1, 1, 1, 1))\n",
    "            ) * epsilon\n",
    "        \n",
    "        # Clip to valid image range\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "        x_adv = x_adv.detach()\n",
    "    \n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluation Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/evaluation/metrics.py\n",
    "def accuracy(model, dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            preds = model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def ece(model, dataloader, device=None, n_bins=15):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    model.eval()\n",
    "    confidences, predictions, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            probs = torch.softmax(model(x), dim=1)\n",
    "            conf, pred = probs.max(1)\n",
    "            confidences.append(conf.cpu())\n",
    "            predictions.append(pred.cpu())\n",
    "            labels.append(y.cpu())\n",
    "    \n",
    "    confidences = torch.cat(confidences)\n",
    "    predictions = torch.cat(predictions)\n",
    "    labels = torch.cat(labels)\n",
    "    \n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    ece_score = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            acc = (predictions[mask] == labels[mask]).float().mean()\n",
    "            conf = confidences[mask].mean()\n",
    "            ece_score += (mask.float().mean()) * abs(acc - conf)\n",
    "    \n",
    "    return ece_score.item()\n",
    "\n",
    "# src/evaluation/gradient_similarity.py\n",
    "def compute_input_gradients(model, dataloader, device=None, max_batches=8):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    model.eval()\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    grads = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device, non_blocking=True).requires_grad_(True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = ce(logits, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        g = x.grad.detach().view(x.size(0), -1).cpu().numpy()\n",
    "        grads.append(g)\n",
    "    \n",
    "    if len(grads) == 0:\n",
    "        return np.zeros((0, 0))\n",
    "    \n",
    "    grads = np.concatenate(grads, axis=0)\n",
    "    return grads\n",
    "\n",
    "def gradient_subspace_similarity(models, dataloader, device=None, max_batches=8):\n",
    "    if device is None:\n",
    "        device = next(models[0].parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    Gvecs = []\n",
    "    for m in models:\n",
    "        g = compute_input_gradients(m, dataloader, device=device, max_batches=max_batches)\n",
    "        Gvecs.append(g.mean(axis=0) if g.size > 0 else np.zeros(1))\n",
    "    \n",
    "    L = len(Gvecs)\n",
    "    S = np.zeros((L, L))\n",
    "    norms = [np.linalg.norm(v) + 1e-12 for v in Gvecs]\n",
    "    \n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            S[i, j] = np.dot(Gvecs[i], Gvecs[j]) / (norms[i] * norms[j])\n",
    "    \n",
    "    return S\n",
    "\n",
    "# src/evaluation/ensemble_variance.py\n",
    "def ensemble_variance(models, dataloader, device=None, max_batches=6):\n",
    "    if device is None:\n",
    "        device = next(models[0].parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    variances = []\n",
    "    Kvals = list(range(1, len(models) + 1))\n",
    "    \n",
    "    for k in Kvals:\n",
    "        batch_vars = []\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            per_losses = []\n",
    "            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            for m in models[:k]:\n",
    "                m.eval()\n",
    "                with torch.no_grad():\n",
    "                    l = ce(m(X), y).cpu().numpy()\n",
    "                    per_losses.append(l)\n",
    "            per_losses = np.stack(per_losses, axis=0)\n",
    "            batch_vars.append(np.mean(np.var(per_losses, axis=0)))\n",
    "        variances.append(np.mean(batch_vars))\n",
    "    \n",
    "    return Kvals, variances\n",
    "\n",
    "# src/evaluation/loss_landscape.py\n",
    "def param_vector(model):\n",
    "    return torch.nn.utils.parameters_to_vector(model.parameters()).detach().clone()\n",
    "\n",
    "def set_param_vector(model, vec):\n",
    "    torch.nn.utils.vector_to_parameters(vec, model.parameters())\n",
    "\n",
    "def loss_on_vector(model, vec, X, y, device='cpu'):\n",
    "    set_param_vector(model, vec)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X.to(device))\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y.to(device)).item()\n",
    "    return loss\n",
    "\n",
    "def scan_1d_loss(model, dataloader, device='cpu', grid_n=21, radius=1.0):\n",
    "    Xs, ys = [], []\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        Xs.append(X); ys.append(y)\n",
    "        if i >= 2: break\n",
    "    X, y = torch.cat(Xs, dim=0), torch.cat(ys, dim=0)\n",
    "    base = param_vector(model).to(device)\n",
    "    D = base.numel()\n",
    "    direction = torch.randn(D, device=device); direction /= direction.norm()\n",
    "    alphas = np.linspace(-radius, radius, grid_n)\n",
    "    losses = np.zeros(grid_n)\n",
    "    for i, a in enumerate(alphas):\n",
    "        vec = base + a*direction\n",
    "        losses[i] = loss_on_vector(model, vec, X, y, device=device)\n",
    "    set_param_vector(model, base)\n",
    "    return alphas, losses\n",
    "\n",
    "def scan_2d_loss(model, dataloader, device='cpu', grid_n=21, radius=1.0):\n",
    "    Xs, ys = [], []\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        Xs.append(X); ys.append(y)\n",
    "        if i >= 2: break\n",
    "    X, y = torch.cat(Xs, dim=0), torch.cat(ys, dim=0)\n",
    "    base = param_vector(model).to(device)\n",
    "    D = base.numel()\n",
    "    dir_a = torch.randn(D, device=device); dir_a /= dir_a.norm()\n",
    "    dir_b = torch.randn(D, device=device); dir_b -= (dir_b.dot(dir_a))*dir_a; dir_b /= dir_b.norm()\n",
    "    alphas = np.linspace(-radius, radius, grid_n)\n",
    "    betas = np.linspace(-radius, radius, grid_n)\n",
    "    losses = np.zeros((grid_n, grid_n))\n",
    "    for i, a in enumerate(alphas):\n",
    "        for j, b in enumerate(betas):\n",
    "            vec = base + a*dir_a + b*dir_b\n",
    "            losses[i,j] = loss_on_vector(model, vec, X, y, device=device)\n",
    "    set_param_vector(model, base)\n",
    "    return alphas, betas, losses\n",
    "\n",
    "# src/evaluation/transferability.py\n",
    "def transferability_matrix(models, dataloader, device=None, max_batches=10, epsilon=8/255, alpha=2/255, iters=10):\n",
    "    if device is None:\n",
    "        device = next(models[0].parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    L = len(models)\n",
    "    counts = np.zeros((L, L))\n",
    "    totals = np.zeros((L, L))\n",
    "    \n",
    "    for i, src_model in enumerate(models):\n",
    "        src_model.eval()\n",
    "        print(f\"Generating adversarial examples from model {i+1}/{L}...\")\n",
    "        \n",
    "        for b, (X, y) in enumerate(dataloader):\n",
    "            if b >= max_batches:\n",
    "                break\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Generate adversarial examples using source model\n",
    "            X_adv = pgd_attack(\n",
    "                src_model, X, y,\n",
    "                epsilon=epsilon,\n",
    "                alpha=alpha,\n",
    "                iters=iters,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Test transferability to all target models\n",
    "            for j, tgt_model in enumerate(models):\n",
    "                tgt_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    preds = tgt_model(X_adv).argmax(dim=1)\n",
    "                    incorrect = (preds != y).sum().item()\n",
    "                    counts[i, j] += incorrect\n",
    "                    totals[i, j] += y.size(0)\n",
    "    \n",
    "    # Compute transferability rates\n",
    "    transferability = counts / (totals + 1e-12)\n",
    "    \n",
    "    return transferability\n",
    "\n",
    "# src/evaluation/adversarial_images.py\n",
    "def generate_adv_examples(src_model, tgt_model, dataloader, device=None, n_samples=24, epsilon=8/255, alpha=2/255, iters=10):\n",
    "    if device is None:\n",
    "        device = next(src_model.parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    src_model.eval()\n",
    "    tgt_model.eval()\n",
    "    imgs = []\n",
    "    cnt = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        X_adv_src = pgd_attack(\n",
    "            src_model, X, y,\n",
    "            epsilon=epsilon,\n",
    "            alpha=alpha,\n",
    "            iters=iters,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Test on target model (optional, for verification)\n",
    "        with torch.no_grad():\n",
    "            _ = tgt_model(X_adv_src)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        X_np = X.cpu().numpy()\n",
    "        X_adv_np = X_adv_src.cpu().numpy()\n",
    "        \n",
    "        for i in range(X_np.shape[0]):\n",
    "            imgs.append((X_np[i], X_adv_np[i]))\n",
    "            cnt += 1\n",
    "            if cnt >= n_samples:\n",
    "                break\n",
    "        if cnt >= n_samples:\n",
    "            break\n",
    "    \n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils/viz_utils.py\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "def save_heatmap(mat, path, title=\"\", cmap=\"viridis\"):\n",
    "    ensure_dir(path)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(mat, cmap=cmap, square=True, annot=True, fmt='.3f')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_contour(alphas, betas, losses, path, title=\"Loss surface\"):\n",
    "    ensure_dir(path)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(betas, alphas, losses, levels=50, cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Beta Direction\", fontsize=12)\n",
    "    plt.ylabel(\"Alpha Direction\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_image_grid(imgs, path, nrow=8, normalize=True, title=None):\n",
    "    ensure_dir(path)\n",
    "    N = len(imgs)\n",
    "    cols = nrow\n",
    "    rows = (N + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    for i in range(len(axes)):\n",
    "        axes[i].axis('off')\n",
    "        if i < N:\n",
    "            img = imgs[i]\n",
    "            if normalize:\n",
    "                img = (img + 1) / 2\n",
    "            if len(img.shape) == 3 and img.shape[0] == 3:\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "            elif len(img.shape) == 3 and img.shape[0] == 1:\n",
    "                img = img[0]\n",
    "            axes[i].imshow(np.clip(img, 0, 1), cmap='gray' if len(img.shape) == 2 else None)\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_line(xs, ys, path, xlabel=\"\", ylabel=\"\", title=\"\"):\n",
    "    ensure_dir(path)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(xs, ys, marker='o', linewidth=2, markersize=6)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Display functions for Colab\n",
    "def display_heatmap(mat, title=\"\", cmap=\"viridis\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(mat, cmap=cmap, square=True, annot=True, fmt='.3f')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_line(xs, ys, xlabel=\"\", ylabel=\"\", title=\"\"):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(xs, ys, marker='o', linewidth=2, markersize=6)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_contour(alphas, betas, losses, title=\"Loss surface\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(betas, alphas, losses, levels=50, cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Beta Direction\", fontsize=12)\n",
    "    plt.ylabel(\"Alpha Direction\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_image_grid(imgs, nrow=8, normalize=True, title=None):\n",
    "    N = len(imgs)\n",
    "    cols = nrow\n",
    "    rows = (N + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    for i in range(len(axes)):\n",
    "        axes[i].axis('off')\n",
    "        if i < N:\n",
    "            img = imgs[i]\n",
    "            if normalize:\n",
    "                img = (img + 1) / 2\n",
    "            if len(img.shape) == 3 and img.shape[0] == 3:\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "            elif len(img.shape) == 3 and img.shape[0] == 1:\n",
    "                img = img[0]\n",
    "            axes[i].imshow(np.clip(img, 0, 1), cmap='gray' if len(img.shape) == 2 else None)\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Complete Experiment\n",
    "\n",
    "This section runs the full EGEAT experiment pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "config = ExperimentConfig(\n",
    "    experiment_name='egeat_colab',\n",
    "    seed=42,\n",
    "    device=None,  # Auto-detect\n",
    "    save_dir='results',\n",
    "    training=TrainingConfig(\n",
    "        batch_size=128,\n",
    "        learning_rate=2e-4,\n",
    "        epochs=5,  # Reduced for Colab demo\n",
    "        lambda_geom=0.1,\n",
    "        lambda_soup=0.05,\n",
    "        epsilon=8/255,\n",
    "        use_mixed_precision=False,\n",
    "        num_workers=2\n",
    "    ),\n",
    "    model=ModelConfig(\n",
    "        model_type='SimpleCNN',\n",
    "        input_channels=3,\n",
    "        num_classes=10,\n",
    "        ensemble_size=3  # Reduced for Colab demo\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        dataset='cifar10',\n",
    "        val_split=0.1,\n",
    "        augment=True\n",
    "    ),\n",
    "    evaluation=EvaluationConfig(\n",
    "        max_batches=10,\n",
    "        n_bins_ece=15,\n",
    "        loss_landscape_grid_n=21,\n",
    "        loss_landscape_radius=1.0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set up directories\n",
    "os.makedirs(config.save_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(config.save_dir, 'figures'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.save_dir, 'adv_images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.save_dir, 'checkpoints'), exist_ok=True)\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "import random\n",
    "random.seed(config.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Get device\n",
    "device = get_device(config.device, verbose=True)\n",
    "\n",
    "print(f\"\\nExperiment Configuration:\")\n",
    "print(f\"  Dataset: {config.data.dataset}\")\n",
    "print(f\"  Model: {config.model.model_type}\")\n",
    "print(f\"  Ensemble Size: {config.model.ensemble_size}\")\n",
    "print(f\"  Epochs: {config.training.epochs}\")\n",
    "print(f\"  Batch Size: {config.training.batch_size}\")\n",
    "print(f\"  Lambda Geom: {config.training.lambda_geom}\")\n",
    "print(f\"  Lambda Soup: {config.training.lambda_soup}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Ensemble Models\n",
    "\n",
    "This section trains each model in the ensemble using EGEAT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(f\"\\nLoading {config.data.dataset} dataset...\")\n",
    "if config.data.dataset.lower() == 'cifar10':\n",
    "    train_loader, val_loader, test_loader = get_cifar10_loaders(\n",
    "        batch_size=config.training.batch_size,\n",
    "        val_split=config.data.val_split,\n",
    "        num_workers=config.training.num_workers,\n",
    "        pin_memory=config.training.pin_memory,\n",
    "        augment=config.data.augment,\n",
    "        seed=config.seed\n",
    "    )\n",
    "    config.model.input_channels = 3\n",
    "    config.model.num_classes = 10\n",
    "elif config.data.dataset.lower() == 'mnist':\n",
    "    train_loader, val_loader, test_loader = get_mnist_loaders(\n",
    "        batch_size=config.training.batch_size,\n",
    "        val_split=config.data.val_split,\n",
    "        num_workers=config.training.num_workers,\n",
    "        pin_memory=config.training.pin_memory,\n",
    "        seed=config.seed\n",
    "    )\n",
    "    config.model.input_channels = 1\n",
    "    config.model.num_classes = 10\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {config.data.dataset}\")\n",
    "\n",
    "print(f\"Dataset loaded. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ensemble models\n",
    "print(f\"\\nInitializing {config.model.ensemble_size} {config.model.model_type} models...\")\n",
    "models = []\n",
    "model_class = SimpleCNN if config.model.model_type == 'SimpleCNN' else DCGAN_CNN\n",
    "\n",
    "for i in range(config.model.ensemble_size):\n",
    "    model = model_class(\n",
    "        input_channels=config.model.input_channels,\n",
    "        num_classes=config.model.num_classes\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "print(f\"Initialized {len(models)} models on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Ensemble Models\n",
    "\n",
    "This section trains each model in the ensemble using EGEAT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Ensemble Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_history = {\n",
    "    'models': [],\n",
    "    'metrics': []\n",
    "}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Model {i+1}/{len(models)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.training.learning_rate)\n",
    "    \n",
    "    model_history = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'adv_loss': [],\n",
    "        'geom_loss': [],\n",
    "        'soup_loss': []\n",
    "    }\n",
    "    \n",
    "    # Collect snapshots from previously trained models for ensemble regularization\n",
    "    ensemble_snapshots = None\n",
    "    if i > 0 and config.training.lambda_geom > 0:\n",
    "        ensemble_snapshots = []\n",
    "        for j in range(i):\n",
    "            snapshot_path = os.path.join(config.save_dir, 'checkpoints', f'model_{j}_final.pt')\n",
    "            if os.path.exists(snapshot_path):\n",
    "                # Create a new model instance and load the state_dict\n",
    "                snapshot_model = model_class(\n",
    "                    input_channels=config.model.input_channels,\n",
    "                    num_classes=config.model.num_classes\n",
    "                )\n",
    "                snapshot_model.load_state_dict(torch.load(snapshot_path, map_location=device))\n",
    "                snapshot_model = snapshot_model.to(device)\n",
    "                snapshot_model.eval()  # Set to eval mode\n",
    "                ensemble_snapshots.append(snapshot_model)\n",
    "    \n",
    "    for epoch in range(config.training.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.training.epochs}\")\n",
    "        \n",
    "        # Train one epoch\n",
    "        metrics = train_egeat_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            lambda_geom=config.training.lambda_geom,\n",
    "            lambda_soup=config.training.lambda_soup,\n",
    "            epsilon=config.training.epsilon,\n",
    "            ensemble_snapshots=ensemble_snapshots,\n",
    "            use_mixed_precision=config.training.use_mixed_precision\n",
    "        )\n",
    "        \n",
    "        model_history['epoch'].append(epoch + 1)\n",
    "        model_history['loss'].append(metrics['loss'])\n",
    "        model_history['adv_loss'].append(metrics['adv_loss'])\n",
    "        model_history['geom_loss'].append(metrics['geom_loss'])\n",
    "        model_history['soup_loss'].append(metrics['soup_loss'])\n",
    "        \n",
    "        print(f\"  Loss: {metrics['loss']:.4f}, \"\n",
    "              f\"Adv: {metrics['adv_loss']:.4f}, \"\n",
    "              f\"Geom: {metrics['geom_loss']:.4f}, \"\n",
    "              f\"Soup: {metrics['soup_loss']:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_dir = os.path.join(config.save_dir, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'model_{i}_epoch_{epoch+1}.pt'))\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'model_{i}_final.pt'))\n",
    "    \n",
    "    training_history['models'].append(model_history)\n",
    "    \n",
    "    # Plot training curves for this model\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_history['epoch'], model_history['loss'], 'o-', label='Total Loss', linewidth=2)\n",
    "    plt.plot(model_history['epoch'], model_history['adv_loss'], 's-', label='Adversarial Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title(f'Model {i+1} - Training Losses', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(model_history['epoch'], model_history['geom_loss'], '^-', label='Geometric Loss', linewidth=2)\n",
    "    plt.plot(model_history['epoch'], model_history['soup_loss'], 'v-', label='Soup Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title(f'Model {i+1} - Regularization Losses', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Metrics\n",
    "\n",
    "This section computes and displays all evaluation metrics and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Ensemble Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Individual model accuracies and ECE scores\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Model Accuracies and ECE Scores...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "accuracies = []\n",
    "ece_scores = []\n",
    "for i, model in enumerate(models):\n",
    "    acc = accuracy(model, test_loader, device=device)\n",
    "    ece_score = ece(model, test_loader, device=device, n_bins=config.evaluation.n_bins_ece)\n",
    "    accuracies.append(acc)\n",
    "    ece_scores.append(ece_score)\n",
    "    print(f\"Model {i+1}: Accuracy={acc:.4f}, ECE={ece_score:.4f}\")\n",
    "\n",
    "results['accuracies'] = accuracies\n",
    "results['ece_scores'] = ece_scores\n",
    "\n",
    "# Display accuracy and ECE bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(1, len(accuracies)+1), accuracies, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracies', fontsize=14)\n",
    "axes[0].set_xticks(range(1, len(accuracies)+1))\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, acc in enumerate(accuracies):\n",
    "    axes[0].text(i+1, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "axes[1].bar(range(1, len(ece_scores)+1), ece_scores, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('ECE Score', fontsize=12)\n",
    "axes[1].set_title('Expected Calibration Error (ECE)', fontsize=14)\n",
    "axes[1].set_xticks(range(1, len(ece_scores)+1))\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, ece in enumerate(ece_scores):\n",
    "    axes[1].text(i+1, ece + 0.005, f'{ece:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies):.4f}  {np.std(accuracies):.4f}\")\n",
    "print(f\"Average ECE: {np.mean(ece_scores):.4f}  {np.std(ece_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient subspace similarity\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Gradient Subspace Similarity...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "sim_matrix = gradient_subspace_similarity(\n",
    "    models, test_loader, device=device, max_batches=config.evaluation.max_batches\n",
    ")\n",
    "\n",
    "display_heatmap(\n",
    "    sim_matrix,\n",
    "    title=\"Gradient Subspace Similarity Matrix\"\n",
    ")\n",
    "\n",
    "results['grad_similarity'] = sim_matrix.tolist()\n",
    "\n",
    "print(f\"\\nAverage similarity: {np.mean(sim_matrix[np.triu_indices(len(sim_matrix), k=1)]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble variance\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Ensemble Variance...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "Kvals, vars_ = ensemble_variance(\n",
    "    models, test_loader, device=device, max_batches=config.evaluation.max_batches\n",
    ")\n",
    "\n",
    "display_line(\n",
    "    Kvals, vars_,\n",
    "    xlabel=\"K (Number of Models)\",\n",
    "    ylabel=\"Variance\",\n",
    "    title=\"Ensemble Variance vs Number of Models\"\n",
    ")\n",
    "\n",
    "results['ensemble_variance'] = {'K': Kvals, 'variance': vars_}\n",
    "\n",
    "print(f\"\\nVariance with {len(models)} models: {vars_[-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transferability matrix\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Transferability Matrix...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "P = transferability_matrix(\n",
    "    models, test_loader, device=device, max_batches=config.evaluation.max_batches,\n",
    "    epsilon=config.attack.epsilon, alpha=config.attack.alpha, iters=config.attack.pgd_iters\n",
    ")\n",
    "\n",
    "display_heatmap(\n",
    "    P,\n",
    "    title=\"Adversarial Transferability Matrix\",\n",
    "    cmap=\"Reds\"\n",
    ")\n",
    "\n",
    "results['transferability'] = P.tolist()\n",
    "\n",
    "print(\"\\nTransferability Matrix:\")\n",
    "print(\"Rows: Source model (attacker)\")\n",
    "print(\"Columns: Target model (victim)\")\n",
    "print(f\"Matrix:\\n{P}\")\n",
    "\n",
    "# Compute average transferability (excluding diagonal)\n",
    "mask = ~np.eye(len(P), dtype=bool)\n",
    "avg_transfer = np.mean(P[mask])\n",
    "print(f\"\\nAverage transferability (off-diagonal): {avg_transfer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial examples\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Generating Adversarial Examples...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "adv_samples = generate_adv_examples(\n",
    "    models[0], models[1] if len(models) > 1 else models[0],\n",
    "    test_loader, device=device, n_samples=16, \n",
    "    epsilon=config.attack.epsilon, alpha=config.attack.alpha, iters=config.attack.pgd_iters\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Images:\")\n",
    "display_image_grid(\n",
    "    [orig for orig, _ in adv_samples[:16]],\n",
    "    nrow=4, title=\"Original Images\"\n",
    ")\n",
    "\n",
    "print(\"\\nAdversarial Examples:\")\n",
    "display_image_grid(\n",
    "    [adv for _, adv in adv_samples[:16]],\n",
    "    nrow=4, title=\"Adversarial Examples\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss landscapes\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Loss Landscapes...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"Computing 1D loss landscape...\")\n",
    "alphas1D, losses1D = scan_1d_loss(\n",
    "    models[0], test_loader, device=device,\n",
    "    grid_n=config.evaluation.loss_landscape_grid_n,\n",
    "    radius=config.evaluation.loss_landscape_radius\n",
    ")\n",
    "\n",
    "display_line(\n",
    "    alphas1D, losses1D,\n",
    "    xlabel=\"Alpha (Direction)\", ylabel=\"Loss\",\n",
    "    title=\"1D Loss Landscape\"\n",
    ")\n",
    "\n",
    "results['loss_1d'] = {'alphas': alphas1D.tolist(), 'losses': losses1D.tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing 2D loss landscape...\")\n",
    "alphas2D, betas2D, losses2D = scan_2d_loss(\n",
    "    models[0], test_loader, device=device,\n",
    "    grid_n=config.evaluation.loss_landscape_grid_n,\n",
    "    radius=config.evaluation.loss_landscape_radius\n",
    ")\n",
    "\n",
    "display_contour(\n",
    "    alphas2D, betas2D, losses2D,\n",
    "    title=\"2D Loss Landscape\"\n",
    ")\n",
    "\n",
    "results['loss_2d'] = {\n",
    "    'alphas': alphas2D.tolist(),\n",
    "    'betas': betas2D.tolist(),\n",
    "    'losses': losses2D.tolist()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Results\n",
    "\n",
    "This section provides a summary of all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results to JSON\n",
    "results_path = os.path.join(config.save_dir, 'results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'config': config.to_dict(),\n",
    "        'training_history': training_history,\n",
    "        'evaluation_results': results\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nResults saved to {results_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nDataset: {config.data.dataset}\")\n",
    "print(f\"Ensemble Size: {len(models)}\")\n",
    "print(f\"\\nModel Accuracies:\")\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"  Model {i+1}: {acc:.4f}\")\n",
    "print(f\"\\nAverage Accuracy: {np.mean(accuracies):.4f}  {np.std(accuracies):.4f}\")\n",
    "\n",
    "print(f\"\\nECE Scores:\")\n",
    "for i, ece in enumerate(ece_scores):\n",
    "    print(f\"  Model {i+1}: {ece:.4f}\")\n",
    "print(f\"\\nAverage ECE: {np.mean(ece_scores):.4f}  {np.std(ece_scores):.4f}\")\n",
    "\n",
    "if 'grad_similarity' in results:\n",
    "    sim_matrix = np.array(results['grad_similarity'])\n",
    "    mask = ~np.eye(len(sim_matrix), dtype=bool)\n",
    "    avg_sim = np.mean(sim_matrix[mask])\n",
    "    print(f\"\\nAverage Gradient Similarity: {avg_sim:.4f}\")\n",
    "\n",
    "if 'transferability' in results:\n",
    "    P = np.array(results['transferability'])\n",
    "    mask = ~np.eye(len(P), dtype=bool)\n",
    "    avg_transfer = np.mean(P[mask])\n",
    "    print(f\"Average Transferability: {avg_transfer:.4f}\")\n",
    "\n",
    "if 'ensemble_variance' in results:\n",
    "    vars_ = results['ensemble_variance']['variance']\n",
    "    print(f\"Final Ensemble Variance: {vars_[-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment Completed Successfully!\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
