{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16da1322",
   "metadata": {},
   "source": [
    "\n",
    "# EGEAT Reproduction Notebook (Lightweight)\n",
    "\n",
    "**Exact Geometric Ensemble Adversarial Training (EGEAT)** — lightweight, self-contained reproduction on `sklearn.digits` (8×8 grayscale) so it runs offline.\n",
    "\n",
    "> This notebook implements the core ideas in the uploaded paper: Exact inner maximization (dual-norm), gradient-space geometric regularization, and ensemble/weight-space smoothing, with figures for gradient similarity, loss landscape slices, adversarial examples, and an ablation.\n",
    "\n",
    "**Caveat:** The original paper reports MNIST/CIFAR-10/DREBIN. Since internet is disabled here, we use `sklearn.digits` to make the code fully runnable. You can switch to MNIST/CIFAR-10 by replacing the dataset loader if you have those locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment & config\n",
    "import math, random, time, copy, itertools\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d979e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data: sklearn digits (1797 samples, 10 classes, 8x8 images)\n",
    "digits = load_digits()\n",
    "X = digits.images.astype(np.float32)   # shape (N, 8, 8)\n",
    "y = digits.target.astype(np.int64)     # shape (N,)\n",
    "\n",
    "# Normalize to [0,1]\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=seed, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1765, random_state=seed, stratify=y_trainval)  # ~0.15 total val\n",
    "\n",
    "# Torch tensors\n",
    "X_train_t = torch.from_numpy(X_train).unsqueeze(1)  # (N,1,8,8)\n",
    "X_val_t   = torch.from_numpy(X_val).unsqueeze(1)\n",
    "X_test_t  = torch.from_numpy(X_test).unsqueeze(1)\n",
    "y_train_t = torch.from_numpy(y_train)\n",
    "y_val_t   = torch.from_numpy(y_val)\n",
    "y_test_t  = torch.from_numpy(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)\n",
    "\n",
    "len_train, len_val, len_test = len(train_ds), len(val_ds), len(test_ds)\n",
    "print(len_train, len_val, len_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small MLP for 8x8 images\n",
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dual-norm exact adversary (first-order exact solution) and attacks\n",
    "def exact_perturbation(x, y, model, loss_fn, eps=0.3, p='linf'):\n",
    "    x = x.detach().clone().to(device)\n",
    "    x.requires_grad_(True)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y.to(device))\n",
    "    loss.backward()\n",
    "    g = x.grad.detach()\n",
    "\n",
    "    if p == 'linf':\n",
    "        delta = eps * g.sign()\n",
    "    elif p == 'l2':\n",
    "        g_flat = g.view(g.size(0), -1)\n",
    "        norm = g_flat.norm(p=2, dim=1, keepdim=True).clamp_min(1e-12)\n",
    "        unit = (g_flat / norm).view_as(g)\n",
    "        delta = eps * unit\n",
    "    elif p == 'l1':\n",
    "        g_flat = g.view(g.size(0), -1)\n",
    "        denom = g_flat.abs().sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "        unit = (g_flat / denom).view_as(g)\n",
    "        delta = eps * unit.sign()\n",
    "    else:\n",
    "        raise ValueError(\"p must be 'linf', 'l2', or 'l1'\")\n",
    "\n",
    "    x_adv = (x + delta).clamp(0.0, 1.0).detach()\n",
    "    return x_adv\n",
    "\n",
    "def fgsm_attack(x, y, model, loss_fn, eps=0.3):\n",
    "    return exact_perturbation(x, y, model, loss_fn, eps=eps, p='linf')\n",
    "\n",
    "def pgd_attack(x, y, model, loss_fn, eps=0.3, alpha=0.1, steps=20, p='linf'):\n",
    "    x0 = x.detach().clone().to(device)\n",
    "    x_adv = x0 + torch.empty_like(x0).uniform_(-eps, eps)\n",
    "    x_adv = x_adv.clamp(0,1).detach()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss = loss_fn(model(x_adv), y.to(device))\n",
    "        loss.backward()\n",
    "        g = x_adv.grad.detach()\n",
    "\n",
    "        if p == 'linf':\n",
    "            x_adv = x_adv + alpha * g.sign()\n",
    "            eta = torch.clamp(x_adv - x0, min=-eps, max=eps)\n",
    "            x_adv = (x0 + eta).clamp(0,1).detach()\n",
    "        elif p == 'l2':\n",
    "            g_flat = g.view(g.size(0), -1)\n",
    "            norm = g_flat.norm(p=2, dim=1, keepdim=True).clamp_min(1e-12)\n",
    "            step = (alpha * g_flat / norm).view_as(g)\n",
    "            x_adv = x_adv + step\n",
    "            d = (x_adv - x0).view(x.size(0), -1)\n",
    "            d_norm = d.norm(p=2, dim=1, keepdim=True).clamp_min(1e-12)\n",
    "            d = d * (eps / d_norm).clamp(max=1.0)\n",
    "            x_adv = (x0 + d.view_as(x)).clamp(0,1).detach()\n",
    "        else:\n",
    "            raise ValueError(\"p must be 'linf' or 'l2' in this PGD\")\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Geometric regularization & ensemble smoothing\n",
    "def batch_input_gradients(models, x, y, loss_fn):\n",
    "    grads = []\n",
    "    for m in models:\n",
    "        m.zero_grad(set_to_none=True)\n",
    "        m.eval()\n",
    "        x_ = x.detach().clone().to(device).requires_grad_(True)\n",
    "        loss = loss_fn(m(x_), y.to(device))\n",
    "        loss.backward()\n",
    "        grads.append(x_.grad.detach())\n",
    "    return grads\n",
    "\n",
    "def cosine_similarity(a, b, eps=1e-12):\n",
    "    a = a.view(a.size(0), -1)\n",
    "    b = b.view(a.size(0), -1)\n",
    "    num = (a*b).sum(dim=1)\n",
    "    den = a.norm(p=2, dim=1)*b.norm(p=2, dim=1) + eps\n",
    "    return (num/den).mean()\n",
    "\n",
    "def geometric_regularizer(models, x, y, loss_fn):\n",
    "    if len(models) < 2:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    grads = batch_input_gradients(models, x, y, loss_fn)\n",
    "    sims = []\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            sims.append(cosine_similarity(grads[i], grads[j]))\n",
    "    return torch.stack(sims).mean()\n",
    "\n",
    "def update_soup(snapshots):\n",
    "    if not snapshots:\n",
    "        return None\n",
    "    base = copy.deepcopy(snapshots[0])\n",
    "    with torch.no_grad():\n",
    "        for p in base.parameters():\n",
    "            p.data.zero_()\n",
    "        for s in snapshots:\n",
    "            for pb, ps in zip(base.parameters(), s.parameters()):\n",
    "                pb.add_(ps.data)\n",
    "        for p in base.parameters():\n",
    "            p.data.div_(len(snapshots))\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop: EGEAT\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EGEATConfig:\n",
    "    epochs: int = 8\n",
    "    eps: float = 0.3\n",
    "    norm: str = \"linf\"\n",
    "    lambda_geom: float = 0.1\n",
    "    lambda_soup: float = 0.001\n",
    "    snapshots_k: int = 4\n",
    "    lr: float = 2e-3\n",
    "\n",
    "def train_egeat(config: EGEATConfig):\n",
    "    model = SmallMLP().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    snapshots = []\n",
    "    soup_model = None\n",
    "    snapshot_every = max(1, config.epochs // config.snapshots_k)\n",
    "\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            xb_adv = exact_perturbation(xb, yb, model, loss_fn, eps=config.eps, p=config.norm)\n",
    "\n",
    "            logits = model(xb_adv)\n",
    "            L_adv = loss_fn(logits, yb)\n",
    "\n",
    "            models_for_geom = [model]\n",
    "            if soup_model is not None:\n",
    "                models_for_geom.append(soup_model)\n",
    "            L_geom = geometric_regularizer(models_for_geom, xb, yb, loss_fn)\n",
    "\n",
    "            if soup_model is not None:\n",
    "                L_soup = torch.tensor(0.0, device=device)\n",
    "                for p, ps in zip(model.parameters(), soup_model.parameters()):\n",
    "                    L_soup = L_soup + (p - ps).pow(2).sum()\n",
    "            else:\n",
    "                L_soup = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = L_adv + config.lambda_geom * L_geom + config.lambda_soup * L_soup\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        if epoch % snapshot_every == 0 or epoch == config.epochs:\n",
    "            snapshots.append(copy.deepcopy(model).to(device))\n",
    "            soup_model = update_soup(snapshots)\n",
    "\n",
    "        val_acc = accuracy(model, val_loader)\n",
    "        print(f\"Epoch {epoch}/{config.epochs}  loss={total_loss/len(train_ds):.4f}  val_acc={val_acc:.3f}  snaps={len(snapshots)}\")\n",
    "\n",
    "    final_soup = update_soup(snapshots) if snapshots else copy.deepcopy(model)\n",
    "    return model, final_soup\n",
    "\n",
    "config = EGEATConfig()\n",
    "egeat_model, egeat_soup = train_egeat(config)\n",
    "print(\"Validation acc (model):\", accuracy(egeat_model, val_loader))\n",
    "print(\"Validation acc (soup): \", accuracy(egeat_soup,  val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline: PGD adversarial training\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PGDConfig:\n",
    "    epochs: int = 8\n",
    "    eps: float = 0.3\n",
    "    alpha: float = 0.1\n",
    "    steps: int = 5\n",
    "    lr: float = 2e-3\n",
    "\n",
    "def train_pgd_adv(config: PGDConfig):\n",
    "    model = SmallMLP().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            xb_adv = pgd_attack(xb, yb, model, loss_fn, eps=config.eps, alpha=config.alpha, steps=config.steps, p='linf')\n",
    "            logits = model(xb_adv)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        val_acc = accuracy(model, val_loader)\n",
    "        print(f\"[PGD] Epoch {epoch}/{config.epochs} loss={total/len(train_ds):.4f} val_acc={val_acc:.3f}\")\n",
    "    return model\n",
    "\n",
    "pgd_model = train_pgd_adv(PGDConfig())\n",
    "print(\"Validation acc (PGD):\", accuracy(pgd_model, val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44451251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation: clean and adversarial accs\n",
    "def eval_adv_acc(model, loader, attack='fgsm', eps=0.3, alpha=0.1, steps=20):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in loader:\n",
    "        if attack == 'fgsm':\n",
    "            xb_adv = fgsm_attack(xb, yb, model, loss_fn, eps=eps)\n",
    "        elif attack == 'pgd':\n",
    "            xb_adv = pgd_attack(xb, yb, model, loss_fn, eps=eps, alpha=alpha, steps=steps, p='linf')\n",
    "        else:\n",
    "            raise ValueError(\"attack must be 'fgsm' or 'pgd'\")\n",
    "        with torch.no_grad():\n",
    "            pred = model(xb_adv.to(device)).argmax(1).cpu()\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct/total\n",
    "\n",
    "clean_e = accuracy(egeat_model, test_loader)\n",
    "fgsm_e  = eval_adv_acc(egeat_model, test_loader, attack='fgsm', eps=0.3)\n",
    "pgd_e   = eval_adv_acc(egeat_model, test_loader, attack='pgd',  eps=0.3, alpha=0.1, steps=20)\n",
    "\n",
    "clean_p = accuracy(pgd_model, test_loader)\n",
    "fgsm_p  = eval_adv_acc(pgd_model, test_loader, attack='fgsm', eps=0.3)\n",
    "pgd_p   = eval_adv_acc(pgd_model, test_loader, attack='pgd',  eps=0.3, alpha=0.1, steps=20)\n",
    "\n",
    "print(\"EGEAT — clean/FGSM/PGD:\", round(clean_e,3), round(fgsm_e,3), round(pgd_e,3))\n",
    "print(\"PGD   — clean/FGSM/PGD:\", round(clean_p,3), round(fgsm_p,3), round(pgd_p,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient similarity heatmap\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "models = [egeat_model.to(device).eval(), egeat_soup.to(device).eval(), pgd_model.to(device).eval()]\n",
    "names  = [\"EGEAT model\", \"EGEAT soup\", \"PGD model\"]\n",
    "\n",
    "xb, yb = next(iter(test_loader))\n",
    "xb, yb = xb.to(device), yb.to(device)\n",
    "grads = []\n",
    "for m in models:\n",
    "    m.zero_grad(set_to_none=True)\n",
    "    x_ = xb.detach().clone().requires_grad_(True)\n",
    "    loss = loss_fn(m(x_), yb)\n",
    "    loss.backward()\n",
    "    grads.append(x_.grad.detach())\n",
    "\n",
    "def pairwise_grad_cosine(grads):\n",
    "    M = len(grads)\n",
    "    mat = torch.zeros(M, M)\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            mat[i,j] = cosine_similarity(grads[i], grads[j]).detach().cpu()\n",
    "    return mat\n",
    "\n",
    "G = pairwise_grad_cosine(grads).numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(G, interpolation='nearest')\n",
    "plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
    "plt.yticks(range(len(names)), names)\n",
    "plt.title(\"Gradient Subspace Cosine Similarity\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/data/fig_gradient_similarity.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss surface slice near EGEAT parameters\n",
    "def get_params_vector(model):\n",
    "    return torch.cat([p.detach().view(-1) for p in model.parameters()])\n",
    "\n",
    "def set_params_vector(model, vec):\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        n = p.numel()\n",
    "        p.data.copy_(vec[idx:idx+n].view_as(p))\n",
    "        idx += n\n",
    "\n",
    "base = copy.deepcopy(egeat_model).to(device).eval()\n",
    "w = get_params_vector(base)\n",
    "d1 = torch.randn_like(w); d2 = torch.randn_like(w)\n",
    "d1 = d1 / d1.norm(); d2 = d2 / d2.norm()\n",
    "\n",
    "grid = 15\n",
    "alphas = torch.linspace(-0.5, 0.5, grid)\n",
    "betas  = torch.linspace(-0.5, 0.5, grid)\n",
    "Z = np.zeros((grid, grid), dtype=np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xb_small, yb_small = xb[:128], yb[:128]\n",
    "    for i, a in enumerate(alphas):\n",
    "        for j, b in enumerate(betas):\n",
    "            tmp = copy.deepcopy(base).to(device)\n",
    "            set_params_vector(tmp, w + a*d1 + b*d2)\n",
    "            z = nn.CrossEntropyLoss()(tmp(xb_small), yb_small).item()\n",
    "            Z[i,j] = z\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(alphas.cpu(), betas.cpu(), Z.T, levels=20)\n",
    "plt.title(\"Loss Surface Slice near EGEAT parameters\")\n",
    "plt.xlabel(\"alpha (dir 1)\")\n",
    "plt.ylabel(\"beta (dir 2)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/data/fig_loss_surface.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize adversarial examples\n",
    "idxs = torch.arange(0, 10)\n",
    "xv = X_test_t[idxs]\n",
    "yv = y_test_t[idxs]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "x_fgsm_pgd = fgsm_attack(xv, yv, pgd_model, loss_fn, eps=0.3).cpu()\n",
    "x_exact_e  = exact_perturbation(xv, yv, egeat_model, loss_fn, eps=0.3, p='linf').cpu()\n",
    "\n",
    "def show_triplet(orig, a, b, title_a=\"FGSM(PGD)\", title_b=\"Exact(EGEAT)\"):\n",
    "    n = orig.size(0)\n",
    "    fig, axes = plt.subplots(3, n, figsize=(1.2*n, 3.6))\n",
    "    for i in range(n):\n",
    "        axes[0,i].imshow(orig[i,0].numpy(), vmin=0, vmax=1)\n",
    "        axes[0,i].axis(\"off\")\n",
    "        axes[1,i].imshow(a[i,0].numpy(), vmin=0, vmax=1)\n",
    "        axes[1,i].axis(\"off\")\n",
    "        axes[2,i].imshow(b[i,0].numpy(), vmin=0, vmax=1)\n",
    "        axes[2,i].axis(\"off\")\n",
    "    axes[0,0].set_ylabel(\"Original\")\n",
    "    axes[1,0].set_ylabel(title_a)\n",
    "    axes[2,0].set_ylabel(title_b)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/mnt/data/fig_adversarial_grid.png\")\n",
    "    plt.show()\n",
    "\n",
    "show_triplet(xv, x_fgsm_pgd, x_exact_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transferability experiment: craft on source, test on target\n",
    "def transfer_rate(source_model, target_model, loader, eps=0.3):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total, fooled = 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb_adv = fgsm_attack(xb, yb, source_model, loss_fn, eps=eps)\n",
    "        with torch.no_grad():\n",
    "            pred_t = target_model(xb_adv.to(device)).argmax(1).cpu()\n",
    "        fooled += (pred_t != yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return fooled/total\n",
    "\n",
    "models = [egeat_model.to(device).eval(), egeat_soup.to(device).eval(), pgd_model.to(device).eval()]\n",
    "names  = [\"EGEAT model\", \"EGEAT soup\", \"PGD model\"]\n",
    "pairs = [(0,1), (0,2), (1,2)]\n",
    "rates = []\n",
    "for i,j in pairs:\n",
    "    r = transfer_rate(models[i], models[j], test_loader, eps=0.3)\n",
    "    rates.append(r)\n",
    "    print(f\"Transfer {names[i]} -> {names[j]}: {r:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "xs = np.arange(len(pairs))\n",
    "plt.bar(xs, rates)\n",
    "plt.xticks(xs, [f\"{names[i]}→{names[j]}\" for i,j in pairs], rotation=30, ha='right')\n",
    "plt.ylabel(\"Transfer Rate\")\n",
    "plt.title(\"Adversarial Transferability (FGSM)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/data/fig_transfer.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ablation (tiny epochs for speed): effect of lambda_geom and lambda_soup\n",
    "import pandas as pd\n",
    "\n",
    "def quick_egeat(epochs, lam_geom, lam_soup):\n",
    "    cfg = EGEATConfig(epochs=4, eps=0.3, norm=\"linf\",\n",
    "                      lambda_geom=lam_geom, lambda_soup=lam_soup,\n",
    "                      snapshots_k=2, lr=2e-3)\n",
    "    m, s = train_egeat(cfg)\n",
    "    clean = accuracy(m, test_loader)\n",
    "    pgd20 = eval_adv_acc(m, test_loader, attack='pgd', eps=0.3, alpha=0.1, steps=20)\n",
    "    # simple entropy proxy (not true ECE)\n",
    "    m.eval(); ent = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            p = F.softmax(m(xb.to(device)), dim=1).cpu().numpy()\n",
    "            ent.append(-(p*np.log(p+1e-12)).sum(axis=1))\n",
    "    ece_proxy = float(np.mean(ent))\n",
    "    return clean, pgd20, ece_proxy\n",
    "\n",
    "settings = [(0.0,0.0), (0.1,0.0), (0.1,0.001)]\n",
    "results = []\n",
    "for lam_g, lam_s in settings:\n",
    "    print(f\"ABLT: lambda_geom={lam_g}, lambda_soup={lam_s}\")\n",
    "    c, p20, ece = quick_egeat(epochs=4, lam_geom=lam_g, lam_soup=lam_s)\n",
    "    results.append((lam_g, lam_s, c, p20, ece))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"lambda_geom\",\"lambda_soup\",\"clean_acc\",\"pgd20_acc\",\"ece_proxy\"])\n",
    "print(df)\n",
    "df.to_csv(\"/mnt/data/ablation_results.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"lambda_geom\"], df[\"pgd20_acc\"], marker=\"o\")\n",
    "plt.xlabel(\"lambda_geom\")\n",
    "plt.ylabel(\"PGD-20 Accuracy\")\n",
    "plt.title(\"Ablation: Effect of geometric regularization\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/data/fig_ablation.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== SUMMARY (Digits dataset) ===\")\n",
    "print(\"Saved figures to /mnt/data/: fig_gradient_similarity.png, fig_loss_surface.png, fig_adversarial_grid.png, fig_transfer.png, fig_ablation.png\")\n",
    "print(\"Ablation CSV: /mnt/data/ablation_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
