# ğŸ§  Exact Geometric Ensemble Adversarial Training (EGEAT)

> **A Unified Framework for Exact Optimization, Gradient-Space Geometry, and Ensemble Robustness**

This repository hosts the official implementation of **EGEAT â€” Exact Geometric Ensemble Adversarial Training**, a unified adversarial learning framework that merges **exact inner maximization**, **gradient-space regularization**, and **ensemble-based smoothing** into one cohesive optimization paradigm.

ğŸ“„ *Companion to the research paper:*  

**"Exact Geometric Ensemble Adversarial Training (EGEAT): A Unified Framework for Robust Optimization and Gradient-Space Regularization" (2024)**

---

## ğŸ“˜ Abstract

**EGEAT** reframes adversarial robustness as a *problem of geometry rather than iteration*.  

It unifies three orthogonal ideas:  

1. **Exact perturbation derivation** via convex duality â€” yielding analytic adversarial directions without multi-step PGD.  

2. **Gradient-space regularization** to decorrelate sensitivity subspaces and suppress transferability.  

3. **Parameter-space ensemble smoothing** to flatten sharp minima and stabilize generalization.

Across CIFAR-10, MNIST, and DREBIN, EGEAT attains PGD-level robustness while reducing computation by **3â€“5Ã—**, improving ensemble diversity, and providing interpretable geometric diagnostics for robustness.

---

## ğŸ§© Method Overview

EGEAT consists of three tightly coupled mechanisms, each addressing a key failure mode of conventional adversarial training.

### 1. **Exact Adversarial Optimization**

   - Derives **closed-form perturbations** under an $\ell_\infty$-bounded constraint using convex duality.  

   - Replaces iterative PGD with analytic updates: $\delta^\star = \epsilon \frac{g}{\|g\|_*}$, where $g=\nabla_x\ell(f_\theta(x),y)$.  

   - **Effect:** Guarantees first-order optimality while eliminating costly inner loops.

### 2. **Geometric Regularization**

   - Penalizes **cosine similarity among gradient subspaces** across ensemble members:

     \[

     \mathcal{L}_{\text{geom}} =

     \sum_{i<j}

     \frac{\mathrm{Tr}(G_i G_j^\top)}{\|G_i\|_F \|G_j\|_F}.

     \]

   - Encourages orthogonality of sensitivity directions, reducing adversarial transfer between models.  

   - Adds negligible computational overhead (~15% per epoch).

### 3. **Ensemble Smoothing**

   - Combines **parameter averaging** and **adversarial weight perturbation** to regularize the optimization path:

     \[

     \mathcal{L}_{\text{soup}} =

     \|\theta - \theta_{\text{soup}}\|_2^2 + \gamma\|\theta - \theta_{\text{AWP}}\|_2^2.

     \]

   - Promotes flatter minima and smoother convergence, improving calibration and stability under distribution shift.

---

## ğŸ§® Unified Objective

\[

\mathcal{L}_{\text{EGEAT}} =

\mathbb{E}_{(x,y)} \Big[

  \mathcal{L}_{\text{CE}}(f_\theta(x+\delta^\star),y)

  + \lambda_1 \mathcal{L}_{\text{geom}}

  + \lambda_2 \mathcal{L}_{\text{soup}}

\Big],

\]

where $\delta^\star = \epsilon g/\|g\|_*$ is the analytic inner maximizer.  

This objective unifies **exact optimization**, **geometric decorrelation**, and **ensemble variance minimization** within a single differentiable loss.  

---

## ğŸ“Š Visual Results

### ğŸ§® Variance vs Ensemble Size â€” *Parameter-Space Stability*

EGEAT exhibits logarithmic variance decay with increasing ensemble size, confirming smoother convergence through weight-space averaging.

![Variance vs Ensemble Size](figures/fig5_variance_surface.png)  

*Figure 1. 3D surface of ensemble variance as a function of ensemble size K and smoothing strength Î»â‚‚.*

---

### ğŸŒŒ 3D Gradient Constellation â€” *Subspace Decorrelation*

PCA projection of input-gradient vectors shows geometric disentanglement: EGEAT ensembles occupy orthogonal gradient subspaces compared to PGD and Soup baselines.

![3D Gradient Constellation](figures/gradient_constellation_3d.png)  

*Figure 2. Gradient-space PCA revealing decorrelated sensitivity subspaces (EGEAT = blue, Soup = orange, PGD = teal).*

---

### ğŸ”ï¸ Loss Landscape â€” *Flatter, More Isotropic Basins*

Loss-surface visualization demonstrates EGEAT's smoother curvature and reduced anisotropy, supporting the geometric-robustness hypothesis.

![Loss Landscape](figures/loss_landscape_showcase.png)  

*Figure 3. 2-D cross-section of the loss surface; darker regions correspond to flatter basins.*

---

### ğŸ” Adversarial Transfer Graph â€” *Reduced Cross-Model Coupling*

Edge thickness encodes attack transfer rate between models; EGEAT minimizes cross-model connectivity.

![Adversarial Transfer Graph](figures/transfer_graph.png)  

*Figure 4. Transfer graph illustrating lowered inter-model transferability under EGEAT.*

---

### ğŸ“ˆ Training Progress â€” *Convergence Dynamics*

Smooth loss descent and monotonic accuracy gain reflect training stability despite adversarial regularization.

![PGD Training Progress](figures/pgd_training_progress.png)  

*Figure 5. Training and validation trajectories for CIFAR-10.*

---

## ğŸš€ Features

- **Analytic adversarial perturbations** â€” provably first-order optimal  

- **Gradient-space orthogonalization** â€” geometric transfer suppression  

- **Parameter-space smoothing** â€” flatter minima, better calibration  

- **Benchmark coverage:** CIFAR-10 / MNIST / DREBIN  

- **Attack suite:** FGSM, PGD, CW-$L_2$  

- **GPU-optimized with mixed precision**

---

## âš™ï¸ Installation

```bash

git clone <repository-url>

cd EGEAT-Experiments

python -m venv venv

source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt

python -c "import torch; print(torch.__version__, torch.cuda.is_available())"

```

---

## ğŸš€ Training

### Standard Run

```bash

python run_experiments.py \

  --dataset cifar10 \

  --model-type SimpleCNN \

  --ensemble-size 5 \

  --lambda-geom 0.1 \

  --lambda-soup 0.05 \

  --epochs 20 \

  --epsilon 0.031 \

  --mixed-precision

```

### Advanced Configuration

```bash

python run_experiments.py \

  --experiment-name egeat_cifar10 \

  --dataset cifar10 \

  --ensemble-size 5 \

  --batch-size 128 \

  --learning-rate 2e-4 \

  --lambda-geom 0.1 \

  --lambda-soup 0.05 \

  --epsilon 0.031 \

  --save-dir results \

  --device cuda:0

```

---

## ğŸ“ Repository Structure

```
EGEAT-Experiments/

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ attacks/          # FGSM, PGD, CW implementations

â”‚   â”œâ”€â”€ evaluation/       # Metrics, gradient similarity, transfer

â”‚   â”œâ”€â”€ models/           # CNN, MLP architectures

â”‚   â”œâ”€â”€ training/         # EGEAT training loop

â”‚   â”œâ”€â”€ utils/            # Data + visualization tools

â”‚   â””â”€â”€ config.py

â”œâ”€â”€ results/

â”‚   â”œâ”€â”€ figures/

â”‚   â”œâ”€â”€ logs/

â”‚   â””â”€â”€ checkpoints/

â”œâ”€â”€ run_experiments.py

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md

```

---

## ğŸ“ˆ Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **Gradient Subspace Similarity** | Mean cosine overlap between model gradients |
| **Ensemble Variance** | Variance of parameters around centroid |
| **Transferability** | Cross-model attack success rate |
| **Loss Curvature** | Hessian trace or sharpness proxy |
| **ECE** | Expected calibration error |
| **Robust Accuracy** | Accuracy under PGD-20 and FGSM |

---

## ğŸ§© Key Findings

- Geometric regularization lowers transferability by **6â€“8%**.

- Ensemble variance decays logarithmically with K, improving stability.

- Closed-form perturbations achieve PGD-level robustness **3â€“5Ã— faster**.

- Combined $\lambda_1$â€“$\lambda_2$ regularization yields smooth, isotropic minima.

---

## ğŸ”¬ Experimental Summary

| Dataset | Model | Attack | Robust Acc (PGD-20) | Î” vs PGD |
|---------|-------|--------|---------------------|----------|
| CIFAR-10 | SimpleCNN | PGD | **0.31** | **+ 4.5 %** |
| MNIST | MLP | FGSM | **0.94** | **+ 2.3 %** |
| DREBIN | MLP | Lâˆ | **0.88** | **+ 3.7 %** |

---

## ğŸ¯ Key Parameters

- `--lambda-geom` = gradient decorrelation weight (default 0.1)

- `--lambda-soup` = ensemble smoothing weight (default 0.05)

- `--epsilon` = perturbation radius (8/255 â‰ˆ 0.031)

- `--ensemble-size` = number of ensemble members (default 5)

- `--model-type` = SimpleCNN | DCGAN_CNN | MLP_Drebin

---

## ğŸ§­ Future Directions

- Higher-order curvature modeling for $\mathcal{O}(\epsilon^2)$ accuracy.

- Adaptive Bayesian ensembles with diffusion-based sampling.

- Cross-domain extensions to multimodal / RL settings.

- Formal convergence analysis and certificate-based robustness.

- Large-scale scaling to ViT and ConvNeXt architectures.

---

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@article{egeat2024,
  title={Exact Geometric Ensemble Adversarial Training (EGEAT): A Unified Framework for Robust Optimization and Gradient-Space Regularization},
  author={Your Name},
  journal={Your Journal},
  year={2024}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“§ Contact

For questions or issues, please open an issue on the repository or contact the maintainers.

---

## ğŸ™ Acknowledgments

- PyTorch team for the excellent deep learning framework
- Adversarial robustness research community
- Contributors and reviewers of this work
